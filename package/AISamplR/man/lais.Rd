% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/adaptive_is.R
\name{pmc}
\alias{pmc}
\alias{apis}
\alias{lais}
\title{Adaptive Importance Sampling (LAIS, APIS, PMC).}
\usage{
pmc(logposterior, mu, sig2_adapt = sig2_samp, sig2_samp,
  compute_logdenom = compute_logdenom_byrow, N = 10, T = 100,
  M = 2)

apis(logposterior, mu, sig2_adapt = sig2_samp, sig2_samp,
  compute_logdenom = compute_logdenom_byrow, N = 10, T = 100,
  M = 2)

lais(logposterior, mu, sig2_adapt = sig2_samp, sig2_samp,
  compute_logdenom = compute_logdenom_byrow, T = 100, N = 2, M = 2)
}
\arguments{
\item{logposterior}{is either of function or a pointer to a C++ function.
Those functions have to take as argument an numerical vector of a given length D.
This argument corresponds to the log-likelihood of the posterior distribution,
we want to draw samples from.
The algorithm used in the adapation step are iterative. 
Therefore, this step can be quite slow in R
dependening on the \code{logposterior} used. 
To improve the computing performance, one can use
a C++ function by providing its pointer.
To see how such pointers can be created with the package \code{Rcpp},
see either the function \code{\link{make_lposterior_rcpp}} or
the example below.}

\item{mu}{A numerical matrix of dimension D x N
used to initialized the location parameters of the N chains of proposal distributions.}

\item{sig2_adapt}{A numerical vector of length D indicating for each dimension
the variance of the gaussian proposal distribution used in the adatation step.}

\item{sig2_samp}{A numerical vector of length D indicating for each dimension
the variance of the gaussian proposal distribution used in the sampling step.}

\item{compute_logdenom}{A function that indicates how to compute denominators
for the weights.}

\item{N}{An integer indicating the number of chains of proposal distributions
that we want to use.}

\item{T}{An integer indicating the number of iterations used for
the adaptation of the N chains of proposal distributions. 
It corresponds to the length of the chain of proposal distributions.}

\item{M}{An integer indicating the numbers of samples
to be drawn for each proposal distribution.}
}
\value{
A list with the following elements. 
\itemize{
  \item{mu}{an array of dimension D x T x N containing the location parameters of
  the poposal distributions used for the importance sampling. 
  This location parameters are given by the adaptation step}
  \item{x}{an array of dimension T x d x M x N containing the samples
  drawn from the proposal distributions in the sampling step.}
  \item{loglik}{an array of dimension T x M x N with
  the loglikelihood of the posterior distribution 
  evaluated for each sample drawn in \code{x}.} 
  \item{logdenom}{an array of dimension T x M x N  with logarithm of 
  the denominators of the weights for each sample drawn in \code{x}.} 
  \item{weight}{an array of dimension T x M x N containing the unormalized weights
  associated with the samples in \code{x}.}
}
}
\description{
\code{lais}, \code{apis} and \code{pmc} respectively return samples
 and the associated weights
 generated by the methods LAIS, APIS and PMC.
}
\details{
These functions perform adaptive important sampling.
They are the implementation of 3 methods:
Layered Adaptive Importance Sampling (LAIS),
Adaptive Population Importance Sampling (APIS)
and Population Monte Carlo (PMC).
Theses 3 implementations follows the same steps:
\enumerate{
  \item the adaptation step where a set of N proposal distributions
   are update at each iteration t for t = 1,..., T.
  \item the sampling step where M samples are drawn for each of the T x N proposals.
  \item the weighting step where all T x N x M samples are given a weight.
}

In these 3 implementations, the proposal distributions are
multivariate gaussian distributions with independent marginals. 
The covariance matrix of theses proposal distribution are all equal
and provided by the user through the argument \code{sig2_samp}. 
The location parameters of the proposal distributions are initialized by the user
and are then updated at each iteration t of the adaptation step. 
The three methods implemented here only differ by the way
the location parameters mu_\{t, n\} of the proposals are updated.
\itemize{
  \item \code{lais} uses Markov chain Monte Carlo
  with the Metropolis-Hasting algorithm. 
  \item \code{apis} draws M samples from the gaussian proposal distribution
  with location parameter mu_\{t-1,n\} and then randomly select
  1 of the M samples as the new location parameter mu_\{t-,n\}.   
  Each sample has a different probability of being chosen.
  This probability depends on the weight given by importance sampling
  when performed only with this proposal distribution.
  \item \code{apis} draws M samples from the gaussian proposal distribution
  with location parameter mu_\{t-1,n\} and then takes 
  the weighted mean of M samples as the new location parameter mu_\{t,n\}.
  The weight of each sample is given by important sampling
  when performed only with this proposal distribution.
}

The weighting step can be performed in different way depending on
how we considered the samples drawn at the sampling step.
We can think that one sample was drawn from its own corresponding proposal distribution 
or that one sample was drawn from a mixture of proposal distributions. 
In this package, three methods to compute the denominator of the weights are available:
\itemize{
  \item \code{\link{compute_logdenom_bybox}} 
  where we considered that on sample x_\{t,n,m\} 
  is drawn from an unique 
  proposal distribution with location parameter mu_\{t,n\}.
  \item \code{\link{compute_logdenom_byrow}}
   where we considered that on sample  x_\{t,n,m\} 
  is drawn from an equiprobable mixture
  of all proposal distributions at time t.
  \item \code{\link{compute_logdenom_bytable}}
   where we considered that on sample x_\{t,n,m\} 
  is drawn from an equiprobable mixture
  of all available proposal distributions,
  for all times t from 1 to T and 
  for the N chains of proposal distribution.
}
}
\examples{
# draw samples from the "banana shaped distribution" defined by the loglikelihood lposterior_6.
lposterior_6 <- function(x){
  x1 <- x[1]
  x2 <- x[2]
  logtarget <-  -1/32 * (4 - 10 * x1 - x2^2)^2 - x1^2/50 - x2^2/50
}

D <- 2
T <- 100
N <- 50
M <- 4

# pmc with an R function for the logposterior
pmc_lp6_r <- pmc(lposterior_6,
    mu = matrix(rnorm(D*N, sd = 3), nrow = D, ncol = N),
    sig2_adapt = rep(1, D), sig2_samp = rep(1, D),
    compute_logdenom = compute_logdenom_byrow,
    N = N, T = T, M = M)
with(pmc_lp6_r, compute_expectation(x, weight)) # theorical value: ~ [-1.09, 0]

# apis with an R function for the logposterior
apis_lp6_r <- apis(lposterior_6,
    mu = matrix(rnorm(D*N, sd = 3), nrow = D, ncol = N),
    sig2_adapt = rep(1, D), sig2_samp = rep(1, D),
    compute_logdenom = compute_logdenom_byrow,
    N = N, T = T, M = M)
with(apis_lp6_r, compute_expectation(x, weight)) # theorical value: ~ [-1.09, 0]

# lais with an R function for the logposterior
lais_lp6_r <- lais(lposterior_6,
    mu = matrix(rnorm(D * N, sd = 3), nrow = D, ncol = N),
    sig2_adapt = rep(1, D), sig2_samp = rep(1, D),
    compute_logdenom = compute_logdenom_byrow,
    N = N, T = T, M = M)
with(lais_lp6_r, compute_expectation(x, weight)) # theorical value: ~ [-1.09, 0]
with(lais_lp6_r, rgl_plot(x[1,,,], x[2,,,], exp(loglik)))
with(lais_lp6_r, rgl_plot(x[1,,,], x[2,,,], sqrt(weight)))

# With C++ code
# Create a C++ function and a pointer to the function 
# logtarget <-  -1/32 * (4 - 10 * x1 - x2^2)^2 - x1^2/50 - x2^2/50
 body_lp6 <- '
 // [[Rcpp::export]]
 double lposterior(NumericVector x){
 double x1 = x[0];
 double x2 = x[1];
 double logtarget = -1.0/32 * pow(4 - 10 * x1 - pow(x2, 2), 2) - pow(x1, 2)/50 - pow(x2, 2)/50;
 return logtarget;
 }'
lp6 <- make_lposterior_rcpp(body = body_lp6)

# lais with an external pointer to an C++ function
lais_lp6_rcpp <-  lais(lp6$pointer,
    mu = matrix(rnorm(D*N, sd = 3), nrow = D, ncol = N),
    sig2_adapt = rep(1, D), sig2_samp = rep(1, D),
    compute_logdenom = compute_logdenom_byrow,
    N = N, T = T, M = M)
with(lais_lp6_rcpp, compute_expectation(x, weight)) # theorical value: ~ [-1.09, 0]
with(lais_lp6_rcpp, rgl_plot(x[1,,,], x[2,,,], exp(loglik)))
with(lais_lp6_rcpp, rgl_plot(x[1,,,], x[2,,,], sqrt(weight)))

}
